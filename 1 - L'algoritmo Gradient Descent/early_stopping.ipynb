{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "early_stopping.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EnFiore/ai-machine-learning-modelli-e-algoritmi/blob/main/1%20-%20L'algoritmo%20Gradient%20Descent/early_stopping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping"
      ],
      "metadata": {
        "id": "u3dTj91rvSKV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OauAm3souuVU"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 1"
      ],
      "metadata": {
        "id": "F5-wyuWnySbW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = make_classification(n_samples=1250, n_features=4, n_informative=2, random_state=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "j8WdDyvQvYYJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "n_batches = 1\n",
        "tol = 0.0001\n",
        "n_iter_no_change = 5\n",
        "n_iter_count = 0\n",
        "\n",
        "batch_size = X_train.shape[0]/n_batches\n",
        "\n",
        "classes = np.unique(Y)\n",
        "\n",
        "sgd = SGDClassifier(loss=\"log_loss\", random_state=RANDOM_SEED)\n",
        "full_losses = []\n",
        "best_loss = 1\n",
        "\n",
        "tick = time()\n",
        "\n",
        "#se dopo un certo ciclo di iterazioni le performance dell'algoritmo non sono miglirate, si intrrompe l'addestramento\n",
        "#altrimenti si aumentano le iterazioni\n",
        "for epoch in range(epochs):\n",
        "        loss = 0.\n",
        "        X_shuffled, y_shuffled = shuffle(X_train, y_train, random_state=RANDOM_SEED)\n",
        "        for batch in range(n_batches):\n",
        "            batch_start = int(batch*batch_size)\n",
        "            batch_end = int((batch+1)*batch_size)\n",
        "            X_batch = X_shuffled[batch_start:batch_end,:]\n",
        "            Y_batch = y_shuffled[batch_start:batch_end]\n",
        "\n",
        "            sgd.partial_fit(X_batch, Y_batch, classes=classes)\n",
        "            loss = log_loss(y_test, sgd.predict_proba(X_test),labels=classes)\n",
        "            full_losses.append(loss)\n",
        "        print(\"Loss all'epoca %d = %.4f (%.5f)\" % (epoch+1, loss, best_loss-loss))\n",
        "\n",
        "\n",
        "        if loss>=best_loss-tol:\n",
        "\n",
        "          if n_iter_count>=n_iter_no_change:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "          else:\n",
        "            n_iter_count+=1\n",
        "        else:\n",
        "            best_loss = loss\n",
        "            n_iter_count = 0\n",
        "\n",
        "print(f\"Addestramento completato in {time()-tick:.2f} secondi\")\n",
        "loss = log_loss(y_test, sgd.predict_proba(X_test))\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTF6CUfxvbw6",
        "outputId": "eabc18be-705d-4d44-b7bf-b61f11805a51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss all'epoca 1 = 0.5685 (0.43154)\n",
            "Loss all'epoca 2 = 0.3823 (0.18613)\n",
            "Loss all'epoca 3 = 0.2312 (0.15113)\n",
            "Loss all'epoca 4 = 0.1895 (0.04172)\n",
            "Loss all'epoca 5 = 0.1603 (0.02918)\n",
            "Loss all'epoca 6 = 0.1376 (0.02270)\n",
            "Loss all'epoca 7 = 0.1341 (0.00348)\n",
            "Loss all'epoca 8 = 0.1271 (0.00699)\n",
            "Loss all'epoca 9 = 0.1225 (0.00461)\n",
            "Loss all'epoca 10 = 0.1199 (0.00260)\n",
            "Loss all'epoca 11 = 0.1184 (0.00153)\n",
            "Loss all'epoca 12 = 0.1174 (0.00095)\n",
            "Loss all'epoca 13 = 0.1168 (0.00065)\n",
            "Loss all'epoca 14 = 0.1163 (0.00048)\n",
            "Loss all'epoca 15 = 0.1159 (0.00038)\n",
            "Loss all'epoca 16 = 0.1156 (0.00032)\n",
            "Loss all'epoca 17 = 0.1153 (0.00028)\n",
            "Loss all'epoca 18 = 0.1151 (0.00025)\n",
            "Loss all'epoca 19 = 0.1149 (0.00024)\n",
            "Loss all'epoca 20 = 0.1146 (0.00023)\n",
            "Loss all'epoca 21 = 0.1144 (0.00022)\n",
            "Loss all'epoca 22 = 0.1142 (0.00022)\n",
            "Loss all'epoca 23 = 0.1140 (0.00021)\n",
            "Loss all'epoca 24 = 0.1138 (0.00021)\n",
            "Loss all'epoca 25 = 0.1136 (0.00020)\n",
            "Loss all'epoca 26 = 0.1134 (0.00020)\n",
            "Loss all'epoca 27 = 0.1132 (0.00019)\n",
            "Loss all'epoca 28 = 0.1130 (0.00019)\n",
            "Loss all'epoca 29 = 0.1128 (0.00019)\n",
            "Loss all'epoca 30 = 0.1126 (0.00018)\n",
            "Loss all'epoca 31 = 0.1124 (0.00018)\n",
            "Loss all'epoca 32 = 0.1123 (0.00017)\n",
            "Loss all'epoca 33 = 0.1121 (0.00017)\n",
            "Loss all'epoca 34 = 0.1119 (0.00017)\n",
            "Loss all'epoca 35 = 0.1118 (0.00016)\n",
            "Loss all'epoca 36 = 0.1116 (0.00016)\n",
            "Loss all'epoca 37 = 0.1114 (0.00016)\n",
            "Loss all'epoca 38 = 0.1113 (0.00015)\n",
            "Loss all'epoca 39 = 0.1111 (0.00015)\n",
            "Loss all'epoca 40 = 0.1110 (0.00014)\n",
            "Loss all'epoca 41 = 0.1109 (0.00014)\n",
            "Loss all'epoca 42 = 0.1107 (0.00014)\n",
            "Loss all'epoca 43 = 0.1106 (0.00013)\n",
            "Loss all'epoca 44 = 0.1105 (0.00013)\n",
            "Loss all'epoca 45 = 0.1103 (0.00013)\n",
            "Loss all'epoca 46 = 0.1102 (0.00012)\n",
            "Loss all'epoca 47 = 0.1101 (0.00012)\n",
            "Loss all'epoca 48 = 0.1100 (0.00012)\n",
            "Loss all'epoca 49 = 0.1098 (0.00011)\n",
            "Loss all'epoca 50 = 0.1097 (0.00011)\n",
            "Loss all'epoca 51 = 0.1096 (0.00011)\n",
            "Loss all'epoca 52 = 0.1095 (0.00010)\n",
            "Loss all'epoca 53 = 0.1094 (0.00010)\n",
            "Loss all'epoca 54 = 0.1093 (0.00010)\n",
            "Loss all'epoca 55 = 0.1092 (0.00020)\n",
            "Loss all'epoca 56 = 0.1091 (0.00009)\n",
            "Loss all'epoca 57 = 0.1090 (0.00018)\n",
            "Loss all'epoca 58 = 0.1090 (0.00009)\n",
            "Loss all'epoca 59 = 0.1089 (0.00017)\n",
            "Loss all'epoca 60 = 0.1088 (0.00008)\n",
            "Loss all'epoca 61 = 0.1087 (0.00016)\n",
            "Loss all'epoca 62 = 0.1086 (0.00008)\n",
            "Loss all'epoca 63 = 0.1086 (0.00015)\n",
            "Loss all'epoca 64 = 0.1085 (0.00007)\n",
            "Loss all'epoca 65 = 0.1084 (0.00014)\n",
            "Loss all'epoca 66 = 0.1083 (0.00007)\n",
            "Loss all'epoca 67 = 0.1083 (0.00013)\n",
            "Loss all'epoca 68 = 0.1082 (0.00006)\n",
            "Loss all'epoca 69 = 0.1082 (0.00013)\n",
            "Loss all'epoca 70 = 0.1081 (0.00006)\n",
            "Loss all'epoca 71 = 0.1080 (0.00012)\n",
            "Loss all'epoca 72 = 0.1080 (0.00006)\n",
            "Loss all'epoca 73 = 0.1079 (0.00011)\n",
            "Loss all'epoca 74 = 0.1079 (0.00005)\n",
            "Loss all'epoca 75 = 0.1078 (0.00010)\n",
            "Loss all'epoca 76 = 0.1078 (0.00005)\n",
            "Loss all'epoca 77 = 0.1077 (0.00010)\n",
            "Loss all'epoca 78 = 0.1077 (0.00014)\n",
            "Loss all'epoca 79 = 0.1076 (0.00005)\n",
            "Loss all'epoca 80 = 0.1076 (0.00009)\n",
            "Loss all'epoca 81 = 0.1075 (0.00013)\n",
            "Loss all'epoca 82 = 0.1075 (0.00004)\n",
            "Loss all'epoca 83 = 0.1075 (0.00008)\n",
            "Loss all'epoca 84 = 0.1074 (0.00012)\n",
            "Loss all'epoca 85 = 0.1074 (0.00004)\n",
            "Loss all'epoca 86 = 0.1073 (0.00007)\n",
            "Loss all'epoca 87 = 0.1073 (0.00011)\n",
            "Loss all'epoca 88 = 0.1073 (0.00003)\n",
            "Loss all'epoca 89 = 0.1072 (0.00007)\n",
            "Loss all'epoca 90 = 0.1072 (0.00010)\n",
            "Loss all'epoca 91 = 0.1072 (0.00013)\n",
            "Loss all'epoca 92 = 0.1071 (0.00003)\n",
            "Loss all'epoca 93 = 0.1071 (0.00006)\n",
            "Loss all'epoca 94 = 0.1071 (0.00009)\n",
            "Loss all'epoca 95 = 0.1071 (0.00012)\n",
            "Loss all'epoca 96 = 0.1070 (0.00003)\n",
            "Loss all'epoca 97 = 0.1070 (0.00005)\n",
            "Loss all'epoca 98 = 0.1070 (0.00008)\n",
            "Loss all'epoca 99 = 0.1070 (0.00010)\n",
            "Loss all'epoca 100 = 0.1069 (0.00002)\n",
            "Loss all'epoca 101 = 0.1069 (0.00005)\n",
            "Loss all'epoca 102 = 0.1069 (0.00007)\n",
            "Loss all'epoca 103 = 0.1069 (0.00009)\n",
            "Loss all'epoca 104 = 0.1068 (0.00011)\n",
            "Loss all'epoca 105 = 0.1068 (0.00002)\n",
            "Loss all'epoca 106 = 0.1068 (0.00004)\n",
            "Loss all'epoca 107 = 0.1068 (0.00006)\n",
            "Loss all'epoca 108 = 0.1068 (0.00008)\n",
            "Loss all'epoca 109 = 0.1067 (0.00010)\n",
            "Loss all'epoca 110 = 0.1067 (0.00012)\n",
            "Loss all'epoca 111 = 0.1067 (0.00002)\n",
            "Loss all'epoca 112 = 0.1067 (0.00003)\n",
            "Loss all'epoca 113 = 0.1067 (0.00005)\n",
            "Loss all'epoca 114 = 0.1067 (0.00007)\n",
            "Loss all'epoca 115 = 0.1066 (0.00008)\n",
            "Loss all'epoca 116 = 0.1066 (0.00010)\n",
            "Early stopping\n",
            "Addestramento completato in 0.51 secondi\n",
            "0.10663066113469352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#con sklearn i parametri si settano nella funzione e sono n_iter_no_change\n",
        "sgf = SGDClassifier(max_iter=200,\n",
        "                    loss=\"log\",\n",
        "                     tol = 0.0001,\n",
        "                     n_iter_no_change = 5,\n",
        "                     random_state=RANDOM_SEED\n",
        "                     )\n",
        "\n",
        "sgd.fit(X_train, y_train)\n",
        "loss = log_loss(y_test, sgd.predict_proba(X_test))\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoLGUXTwxtPV",
        "outputId": "66c30170-d4b3-4f6d-b5fd-e53edfdc96c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11296319919238175\n"
          ]
        }
      ]
    }
  ]
}